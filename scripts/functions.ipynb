{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa17919",
   "metadata": {},
   "source": [
    "# Utility Functions for PR104"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141fd883",
   "metadata": {},
   "source": [
    "1. [ann_cv()](#ann)\n",
    "1. [arff_tocsv()](#arff_tocsv)\n",
    "1. [compare_classifiers()](#compare_class)\n",
    "1. [compare_roc()](#compare_roc)\n",
    "1. [data_barplot()](#barplot)\n",
    "1. [data_summary()](#data_summary)\n",
    "1. [df_display()](#df_display)\n",
    "1. [nbayes_cv()](#naivebayes)\n",
    "1. [knn_cv()](#knn)\n",
    "1. [tree_cv()](#tree)\n",
    "1. [plot_line()](#plot_line)\n",
    "1. [plot_performance()](#plotperf)\n",
    "1. [roc_cv()](#roc_cv)\n",
    "1. [save_fig()](#save_fig)\n",
    "1. [scatter_custom()](#scatter_cust)\n",
    "1. [svm_cv()](#svm)\n",
    "1. [varselect_tocsv()](#varselect_tocsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6fa7955",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a2e96b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9a3916",
   "metadata": {},
   "source": [
    "### ann_cv(*df, ax = None, cbar = False, normalize = None, folds = 10, shuffle = True, seed = 42, n_iter = 50, **kargs*)<a class='anchor' id='ann'></a>\n",
    "#### A wrapper function that performs cross-validated tuning and evaluation of an ANN with 1 hidden layer on a given dataset\n",
    "\n",
    "The function implements a randomized search over the parameter space using `RandomizedSearchCV`. In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the `n_iter` parameter. This way, increasing `n_iter` will always lead to a finer search.\n",
    "\n",
    "For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified. \n",
    "For continuous parameters, such as $\\alpha$ in this case, it is important to specify a continuous distribution to take full advantage of the randomization. A continuous log-uniform random variable is available through `loguniform`. This is a continuous version of log-spaced parameters and is useful for searching penalty values as we often explore values at different orders of magnitude, at least as a first step. For example to specify $\\alpha$ above, `loguniform(0.0001, 0.01)` can be used instead of `[0.0001, 0.001, 0.01]` \n",
    "\n",
    "The function has the following parameters:\n",
    "\n",
    "- **`df`**: Dataframe containing the data.\n",
    "- **`ax`**: *Axes object, default = None* \n",
    "     <br> Axes on which to draw the plot of the confusion matrix.\n",
    "- **`cbar`**: *bool, default = False*\n",
    "     <br> Whether or not to display the colorbar.\n",
    "- **`normalize`**: *None or {'true', 'pred', 'all'}*\n",
    "    <br> Normalization mode to apply to the confusion matrix.\n",
    "- **`folds`**: *int, default = 10*\n",
    "    <br> Number of folds to use in the k-fold cross-validation.\n",
    "- **`shuffle`**: *bool, default = True*\n",
    "    <br> Whether or not to shuffle the data before applying k-fold cross-validation.\n",
    "- **`seed`**: *int, default = 42*\n",
    "    <br> Integer used as a seed for the random number generator\n",
    "- **`n_iter`**: *int, default = 50*\n",
    "    <br> Number of parameter settings that are sampled. Tunes the trade-off runtime vs quality of the solution.\n",
    "- **`**kargs`**: \n",
    "    <br> Additional keyword arguments to pass to the `MLPClassifier`.\n",
    "\n",
    "The function outputs a dictionary containing the following:\n",
    "\n",
    "- **`ConfusionMatrixDisplay`**: *ConfusionMatrixDisplay object*\n",
    "    <br> Object containing the confusion matrix, labels and the Confusion Matrix visualization.\n",
    "- **`confusionmatrix`**: *ndarray of shape (n_classes, n_classes)*\n",
    "  <br> Matrix whose *i*-th row and *j*-th column entry indicates the # of samples with true label being *i*-th class and predicted label being *j*-th class.\n",
    "- **`results`**: *dict of ndarrays*\n",
    "    <br> a dictionary that summarizes the results of cross-validation for each hyperparameter combination tried during the random search.\n",
    "- **`accuracy`**: *float*\n",
    "    <br>Mean cross-validated accuracy of the model.\n",
    "- **`recall`**: *float*\n",
    "    <br>Mean cross-validated percentage recall of the model.\n",
    "- **`auc`**: *float*\n",
    "    <br>Mean cross-validated AUC of the ROC of the model.\n",
    "- **`model`**: *sklearn.pipeline.Pipeline object*\n",
    "    <br>The best estimator from `RandomizedSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6800857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_cv(df, ax = None, cbar = False, normalize = None, folds = 10, shuffle = True, seed = 42, n_iter = 50, **kargs):\n",
    "    \n",
    "    # If no Axes object is provided, create a new one\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    # Check if ax is an instance of the Axes class\n",
    "    elif not isinstance(ax, Axes):\n",
    "        raise TypeError(\"The 'ax' argument must be an Axes object\")\n",
    "        \n",
    "    # Separate the target variable from predictors\n",
    "    y = df['defects']\n",
    "    X = df.drop('defects', axis = 1)\n",
    "    predictors = X.columns.values.tolist()\n",
    "    \n",
    "    # Create StratifiedKFold object\n",
    "    # If shuffle is False, ignore any passed seed to avoid a ValueError\n",
    "    kf = StratifiedKFold(n_splits = folds, shuffle = shuffle, random_state = seed if shuffle else None)\n",
    "\n",
    "    # Create a MLPClassifier model\n",
    "    ann = MLPClassifier(random_state = seed, **kargs)\n",
    "    \n",
    "    # Create column transformer to normalize predictors\n",
    "    z = make_column_transformer((RobustScaler(), predictors), remainder = \"passthrough\")\n",
    "    # Create pipeline\n",
    "    pipe = make_pipeline(z, ann)\n",
    "    \n",
    "    # Perform Randomized Search to find the optimal parameters values\n",
    "    space = {\n",
    "        'mlpclassifier__hidden_layer_sizes': [i for i in range(3,26,2)], # number of neurons in the hidden layer\n",
    "    'mlpclassifier__activation':['logistic', 'tanh' , 'relu'], # Activation function for the hidden layer\n",
    "    'mlpclassifier__alpha': loguniform(0.0001, 0.01) # Strength of the L2 regularization term\n",
    "    }\n",
    "    metrics = ('accuracy','balanced_accuracy','recall','roc_auc')\n",
    "    rnd_srch = RandomizedSearchCV(pipe, space, scoring = metrics, cv = kf, refit = 'balanced_accuracy', n_iter = n_iter,\n",
    "                                 n_jobs = -1, verbose = 0, random_state = seed)\n",
    "    rnd_srch.fit(X, y)\n",
    "    \n",
    "    # Retrieve the mean cross-validated accuracy and recall\n",
    "    acc = rnd_srch.cv_results_['mean_test_accuracy'][rnd_srch.best_index_]\n",
    "    recall_perc = rnd_srch.cv_results_['mean_test_recall'][rnd_srch.best_index_]*100\n",
    "    auc = rnd_srch.cv_results_['mean_test_roc_auc'][rnd_srch.best_index_]\n",
    "    \n",
    "    # Generate cross-validated predicted labels for the target class\n",
    "    y_pred = cross_val_predict(rnd_srch.best_estimator_, X, y, cv=kf)\n",
    "    \n",
    "    \n",
    "    # Create confusion matrix\n",
    "    lab = ['Clean','Buggy']\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(y_true = y, y_pred = y_pred,  cmap = 'Blues', display_labels = lab, \n",
    "                                                   normalize = normalize, ax = ax, colorbar = cbar)\n",
    "    ax.set_title('ANN', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Create output\n",
    "    out = {'ConfusionMatrixDisplay' : disp,\n",
    "           'confusionmatrix' : disp.confusion_matrix,\n",
    "           'results' : rnd_srch.cv_results_,\n",
    "           'accuracy' : round(acc, 3),\n",
    "           'recall' : round(recall_perc, 2),\n",
    "           'auc' : round(auc, 3),\n",
    "           'model' : rnd_srch.best_estimator_}\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b58234c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b0b16f",
   "metadata": {},
   "source": [
    "### arff_tocsv(*name*) <a class='anchor' id='arff_tocsv'></a>\n",
    "#### Loads *.arff* data and save it in a CSV file\n",
    "This function is used to load data from an ARFF file and save it as a CSV file. The `with` statement is used to open the ARFF file, read its contents using the `loadarff` function, and automatically close the file when the with block ends. This ensures that the file is properly closed and avoids potential issues with file handles being left open.\n",
    "The function also checks if there is any missing value in the dataset and if there are missing values it impute the missing values with median values of the same column.\n",
    "It then prints a message indicating that the ARFF file was successfully loaded and saves the DataFrame as a CSV file using the `to_csv()` method. The CSV file is saved in the same directory as the ARFF file, using the same name as the ARFF file, overwriting any existing data. The function then returns the final dataframe.\n",
    "\n",
    "The function takes one argument:\n",
    "\n",
    "- **`name`**: a string that specifies the name of the ARFF file to be loaded. This string should be a valid file name and should have .arff appended to it when the file is read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "facdb089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.arff import loadarff \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def arff_tocsv(name):\n",
    "    # Constructs the full file path for the ARFF file\n",
    "    path = os.path.join(DATA_PATH, name + \".arff\")\n",
    "    \n",
    "    try:\n",
    "        # Reads the data from the file into a NumPy structured array\n",
    "        with open(path, \"r\") as arff_file:\n",
    "            raw_data = loadarff(arff_file)\n",
    "    except FileNotFoundError:\n",
    "        print(\"The specified ARFF file was not found.\")\n",
    "        return\n",
    "    \n",
    "    # Define the columns that should be of integer type\n",
    "    intdict = {'loc':'int','v(g)':'int','ev(g)':'int',\n",
    "               'iv(g)': 'int','n':'int','lOCode':'int',\n",
    "               'lOComment':'int','lOBlank':'int',\n",
    "               'lOCodeAndComment':'int','uniq_Op':'int',\n",
    "               'uniq_Opnd':'int','total_Op':'int',\n",
    "               'total_Opnd':'int','branchCount':'int',\n",
    "               'defects':'int'}\n",
    "    \n",
    "    # Array is then converted to a Pandas dataframe \n",
    "    df_data = pd.DataFrame(raw_data[0])\n",
    "    \n",
    "    # Check if there are any missing values in the dataframe\n",
    "    # 5 rows with missing values can be found in jm1.arff data source\n",
    "    if df_data.isnull().any().any():\n",
    "        # Impute missing values with median of the same column\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        df_data_num = df_data.select_dtypes(include=[np.number])\n",
    "        imputer.fit(df_data_num)\n",
    "        X = imputer.transform(df_data_num)\n",
    "        df_data = pd.DataFrame(X, columns=df_data.columns)\n",
    "    \n",
    "    # Convert the variables contained in the intdict dictionary to an integer dtype\n",
    "    df_data = df_data.astype(dtype=intdict)\n",
    "\n",
    "    print(name + \".arff\",\"successfully loaded\")\n",
    "    \n",
    "    # Saves the dataframe as a CSV file using the same name as the ARFF file\n",
    "    # (any existing data is overwritten)\n",
    "    df_data.to_csv(f'{DATA_PATH}/{name}.csv', mode=\"w\")\n",
    "    print(\"Saved in\", f'{DATA_PATH}/{name}.csv')\n",
    "    return df_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b78543",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7851c1dd",
   "metadata": {},
   "source": [
    "### compare_classifiers(*df, models_list, outname, cbar = False, normalize = None, folds = 10, shuffle = True, seed = 42*) <a class='anchor' id='compare_class'></a>\n",
    "####    Compare classifiers by generating side-by-side confusion matrices visualizations and evaluating their accuracy and recall. Results are saved in a pickle file and returned as a dictionary.\n",
    "\n",
    "The function has the following parameters:\n",
    "\n",
    "- **`df`**: *pandas dataframe*\n",
    "    <br> Input data to apply the machine learning models on.\n",
    "- **`models_list`**: *list of dictionaries*\n",
    "    <br> List of dictionaries containing the classification models (with optional parameters) to apply on the input data.\n",
    "- **`outname`**: *string*\n",
    "    <br> Output name for the results pickle file.\n",
    "- **`cbar`**: *bool, default = False*\n",
    "    <br> Whether or not to display color bar in the confusion matrix.\n",
    "- **`normalize`**: *None or {'true', 'pred', 'all'}*\n",
    "    <br> Normalization mode to apply to the confusion matrix.\n",
    "- **`folds`**: *int, default = 10*\n",
    "    <br> Number of folds for cross-validation.\n",
    "- **`shuffle`**: *bool, default = True*\n",
    "    <br> Whether to shuffle the input data before applying the cross-validation.\n",
    "- **`seed`**: *int, default = 42*\n",
    "    <br> Seed for the random number generator.\n",
    "\n",
    "The function outputs a dictionary containing the following:\n",
    "\n",
    "- **`res`**: *dict*\n",
    "    <br> A dictionary containing selected results under the following keys:\n",
    "       - models: A list of fitted classifier models.\n",
    "       - accuracy: An array of accuracy scores for each model.\n",
    "       - recall: An array of recall scores for each model.\n",
    "       - auc: An array of AUC scores for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5ab2bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_classifiers(df, models_list, outname, cbar = False, normalize = None, folds = 10, shuffle = True, seed = 42):\n",
    "    \n",
    "    try:\n",
    "        full_res = {} # dict to store results of all models\n",
    "        models = copy.deepcopy(models_list) # creating a copy of models_list\n",
    "        n_mod = len(models) # number of models\n",
    "        accuracies = np.empty(n_mod) # empty array to store accuracy of each model\n",
    "        recalls = np.empty(n_mod) # empty array to store recall of each model\n",
    "        aucs = np.empty(n_mod) # empty array to store recall of each model\n",
    "        fitted = [] # list to store fitted models\n",
    "        \n",
    "        # Creates a subplot grid with 1 row and variable number of columns\n",
    "        fig, ax = plt.subplots(1, n_mod, figsize=((n_mod*5), 4)) \n",
    "        \n",
    "        # Loop through all models\n",
    "        for i, model_data in enumerate(models):\n",
    "            model = model_data.pop('model')\n",
    "            \n",
    "            # Check if the model is callable\n",
    "            if not callable(model):\n",
    "                raise TypeError(f\"The 'model' attribute in models_list[{i}] is not callable: {model}\")\n",
    "            \n",
    "            # Call the model with the required arguments and any additional arguments\n",
    "            out = model(df, ax=ax[i], cbar = cbar, normalize = normalize, folds = folds, shuffle = shuffle, seed = seed, **model_data)\n",
    "            \n",
    "            # Get the classifier name\n",
    "            if isinstance(out['model'], Pipeline):\n",
    "                name = out['model'].steps[-1][1].__class__.__name__ \n",
    "            else:\n",
    "                name = out['model'].__class__.__name__\n",
    "            \n",
    "            # Store results of this model in the dict\n",
    "            full_res[name] = out\n",
    "            \n",
    "            # Append the fitted model to the list\n",
    "            fitted.append(out['model'])\n",
    "            \n",
    "            # Store accuracy and recall of this model\n",
    "            accuracies[i] = out['accuracy']\n",
    "            recalls[i] = out['recall']\n",
    "            aucs[i] = out['auc']\n",
    "\n",
    "        # Save results to a binary file\n",
    "        with open(f'{RESULTS_PATH}/{outname}.pickle', 'wb') as handle:\n",
    "            pickle.dump(full_res, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # Store models, accuracy and recall in a dict\n",
    "        res = {'models':fitted, 'accuracy':accuracies, 'recall':recalls, 'auc':aucs}\n",
    "        return  res\n",
    "    \n",
    "    except Exception as e:\n",
    "        # If an error occurs, close the plot and return None\n",
    "        print(f\"An error occurred while processing models_list: {e}\")\n",
    "        plt.close()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea01949",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558b25c7",
   "metadata": {},
   "source": [
    "### compare_roc(*df, models_list, palette = None, **kargs*) <a class='anchor' id='compare_roc'></a>\n",
    "#### A function that compares the ROC curves of multiple binary classifiers.\n",
    "\n",
    "The function has the following parameters:\n",
    "\n",
    "- **`df`**: *Pandas DataFrame*\n",
    "    <br> Dataframe containing features and target variables. The target variable should be binary.\n",
    "- **`models_list`**: *list*\n",
    "    <br> A list of binary classifiers. Each classifier in the list must have `fit` and `predict_proba` methods.\n",
    "- **`**kargs`**: any other keyword arguments to pass to the `roc_cv` function.\n",
    "- **`palette`**: an optional parameter to specify a color palette for the ROC curve plots. The default value is `None`, which means that the default palette `'tab10'` from Matplotlib will be used.\n",
    "    \n",
    "\n",
    "The function outputs the following:\n",
    "\n",
    "- **`aucs`**: *Numpy Array*\n",
    "  <br> An array containing the AUC (Area Under the Curve) values of each binary classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82888870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_roc(df, models_list, palette = None, **kargs):\n",
    "    # Get the number of models\n",
    "    n_mod = len(models_list)\n",
    "    \n",
    "    # Initialize an empty array to store the AUC of each model\n",
    "    aucs = np.empty(n_mod)\n",
    "    \n",
    "    # Create a figure and axis for plotting the ROC curve\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Get a color palette to use for plotting the ROC curves\n",
    "    if palette is None:\n",
    "        # Use the default color palette 'tab10' from Matplotlib \n",
    "        palette = plt.get_cmap('tab10')\n",
    "    # Select a # of colors from the palette equal to the number of ROC curves being plotted\n",
    "    colors = [palette(i) for i in range(n_mod)]\n",
    "    \n",
    "    # Loop through each model in the models_list\n",
    "    for i, model in enumerate(models_list):\n",
    "        try:\n",
    "            # Call the roc_cv function on the current model, passing the df data, axis, and color\n",
    "            out = roc_cv(model, df, ax=ax, chance=False, color=colors[i], **kargs)\n",
    "            # Store the AUC of the current model in the aucs array\n",
    "            aucs[i] = out['AUC']\n",
    "        except Exception as e:\n",
    "            print(f\"Model {i} cannot be fit or predict_proba method is missing: {e}\")\n",
    "            aucs[i] = np.nan\n",
    "            \n",
    "    # Plot the chance level (AUC = 0.5) on the same axis\n",
    "    ax.plot([0, 1], [0, 1], \"k--\", label=\"Chance Level (AUC = 0.5)\", alpha=0.4)\n",
    "    \n",
    "    # Add a legend to the plot to show the AUC for each model\n",
    "    plt.legend(loc='lower right', markerscale=9, prop={'size': 9})\n",
    "    \n",
    "    # Return the AUCs of all the models\n",
    "    return aucs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ed5366",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61951079",
   "metadata": {},
   "source": [
    "### data_barplot(*df, save = True, id = 'data_barplot', **kargs*) <a class='anchor' id='barplot'></a>\n",
    "#### Creates a grouped bar plot with the *Buggy* and *Clean* instances\n",
    "\n",
    "It plots the '*Buggy*' and '*Clean*' columns from `df`, with the '*Buggy*' bars shown in red and the '*Clean*' bars shown in blue. The x-axis of the plot is labeled with the index values from `df`, corresponding to the different datasets employed. The figure has a title, and is saved with the specified `id` before being shown. The `save_fig()` function is a separate function used to save the figure.\n",
    "The function takes four arguments:\n",
    "<br>\n",
    "- **`df`**: is the dataframe containing the data to be plotted\n",
    "- **`save`**: a boolean value indicating whether the plot should be saved (default is True)\n",
    "- **`id`**: a string value specifying the name of the file to save the plot as (default is 'data_barplot')\n",
    "- **`**kargs`**: any other keyword arguments to pass to the save_fig function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e49facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_barplot(df, save = True, id='data_barplot', **kargs):\n",
    "    \n",
    "    # Create x-axis values\n",
    "    num_rows = df.shape[0]\n",
    "    x = np.arange(num_rows)\n",
    "\n",
    "    y1 = df['Buggy']\n",
    "    y2 = df['Clean']\n",
    "    # Set width of bars in plot\n",
    "    width = 0.40\n",
    "\n",
    "    # Set tick positions on x-axis\n",
    "    ticks_pos = [r for r in range(num_rows)]\n",
    "\n",
    "    # Create figure and axis objects\n",
    "    figure, ax = plt.subplots(figsize=(9,4))\n",
    "\n",
    "    # Plot data in grouped bar format\n",
    "    ax.bar(x-0.2, y1, width, label='Buggy', color='#8b0000')\n",
    "    ax.bar(x+0.2, y2, width, label='Clean', color='#00008b')\n",
    "\n",
    "    # Set tick positions and labels on x-axis\n",
    "    ax.set_xticks(ticks_pos, df.index)\n",
    "    \n",
    "    # Get current y-axis ticks\n",
    "    yticks = ax.get_yticks()\n",
    "\n",
    "    # Modify some of the y-axis ticks\n",
    "    yticks[0] = 500\n",
    "    ax.set_yticks(yticks)\n",
    "    \n",
    "    # Remove the horizontal grid lines\n",
    "    ax.xaxis.grid(False)\n",
    "\n",
    "    # Add legend to plot\n",
    "    ax.legend()\n",
    "\n",
    "    # Add title\n",
    "    figure.suptitle('Figure 2: Class Distribution among data - \"Defects\"',\n",
    "               fontsize=12, fontweight='bold')\n",
    "\n",
    "    # Save Figure\n",
    "    if save:\n",
    "        save_fig(id, **kargs)\n",
    "    \n",
    "    # Show plot\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed07b54",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370d3869",
   "metadata": {},
   "source": [
    "### data_summary(*filename, index, plot =* True)<a class='anchor' id='data_summary'></a>\n",
    "#### Reads the data from a specified file, creates a summary table and a grouped bar plot of the data.\n",
    "\n",
    "This function takes three arguments:\n",
    "<br>\n",
    "- **`filename`**: a string that specifies the name of the file containing the data. This function assumes that the file is a CSV file located in the `DATA_PATH` directory.\n",
    "- **`index`**: specifies the column to use as the index of the dataframe.\n",
    "- **`plot`**: a boolean that, if set to True, causes the function to also return a grouped bar plot of the data using the `data_barplot()` function. If plot is not provided, it defaults to True.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30d34846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_summary(filename, index, plot = True):\n",
    "    try:\n",
    "        # Check that the filename is a string\n",
    "        if not isinstance(filename, str):\n",
    "            raise ValueError('ERROR: The filename must be a string')\n",
    "\n",
    "        # Constructs the full file path for the CSV file\n",
    "        path = os.path.join(DATA_PATH, filename + \".csv\")\n",
    "\n",
    "        # Open the file with the given filename and read the data into a DataFrame\n",
    "        table1 = pd.read_csv(path)\n",
    "\n",
    "        # Check that the index is a proper column name\n",
    "        if index not in table1.columns:\n",
    "            raise ValueError('ERROR: The index must be an existing column')\n",
    "\n",
    "        # Set the 'Name' column as the index\n",
    "        table1.set_index(index, inplace=True)\n",
    "\n",
    "        # Add a new column called 'Clean' with the number of modules without bugs\n",
    "        table1.insert(2,'Clean',table1['Instances'] -  table1['Buggy'])\n",
    "        \n",
    "        # Add a new column called 'Imbalance Ratio'\n",
    "        table1.insert(3, 'Imbalance Ratio', table1['Buggy'] / (table1['Clean']))\n",
    "\n",
    "        # Round the values in the 'Imbalance Ratio' column to 2 decimal places\n",
    "        table1['Imbalance Ratio'] = table1['Imbalance Ratio'].round(3)\n",
    "\n",
    "        # Format the values in the 'Imbalance Ratio' column as strings with 2 decimal places\n",
    "        table1['Imbalance Ratio'] = table1['Imbalance Ratio'].map('{:.3f}'.format)\n",
    "\n",
    "        # Convert the dataframe to an HTML table with a caption\n",
    "        html_table1 = table1.style.to_html(caption=\"Table 1: Summary of Datasets\")\n",
    "\n",
    "\n",
    "        # If the plot argument is True, create a grouped bar plot of the data\n",
    "        if plot:\n",
    "            return HTML(html_table1), data_barplot(table1)\n",
    "        else:\n",
    "            # Otherwise, only return the HTML table\n",
    "            return HTML(html_table1)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # If the file does not exist, print an error message\n",
    "        print('ERROR: The file does not exist')\n",
    "    except ValueError as error:\n",
    "        print(error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c31cb12",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d7ead",
   "metadata": {},
   "source": [
    "### df_display(*df, title, decimals = 3, highlight = False*) <a class='anchor' id='df_display'></a>\n",
    "#### Takes a pandas DataFrame and returns a styled version of the DataFrame using the `.style` attribute.\n",
    "\n",
    "This is a Python function that utilizes the styling capabilities of the pandas library's DataFrame object. The function takes the following parameters:\n",
    "\n",
    "- **`df`**: Pandas DataFrame object\n",
    "- **`title`**: string that represents the caption to be set for the styled DataFrame\n",
    "- **`decimals`**: *int, default 3*\n",
    "    <br> Determines the number of decimal places to be displayed in the styled DataFrame\n",
    "- **`highlight`**: *bool, default False*\n",
    "    <br> Indicates whether to highlight the maximum and minimum values in each row of the DataFrame\n",
    "    \n",
    "The function returns a styled version of the input DataFrame df by setting various display properties, including:\n",
    "\n",
    "- the caption of the styled DataFrame, which is set to title and has a font size of 14 points\n",
    "- the cell font size of the styled DataFrame, which is set to 13 points\n",
    "- the number of decimal places displayed in the styled DataFrame, which is set to `decimals`\n",
    "\n",
    "If the highlight argument is set to `True`, the function also highlights the maximum and minimum values in each row of the DataFrame using green and red colors. The styling is performed using the `.style` attribute of the input DataFrame and the `.highlight_max()` and `.highlight_min()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c25be7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_display(df, title, decimals = 3, highlight = False):\n",
    "    \n",
    "    # Create a style object with the specified formatting options\n",
    "    style = (\n",
    "        df.style\n",
    "        .set_caption(title)                 # Set the caption for the table\n",
    "        .set_precision(decimals)            # Set the number of decimal places to display\n",
    "        .set_properties(**{'font-size': '13pt'})  # Set the font size for the cells\n",
    "        .set_table_styles([{'selector': 'caption', 'props': [('font-size', '14pt')]}])  # Set the font size for the caption\n",
    "    )\n",
    "    \n",
    "    # If highlight is True, apply the highlight_max and highlight_min methods to the style object\n",
    "    if highlight:\n",
    "        style = style.highlight_max(axis=1, color='#90EE90').highlight_min(axis=1, color='#FFB6C1')\n",
    "        \n",
    "    return style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40d5aba",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1dd141",
   "metadata": {},
   "source": [
    "### nbayes_cv(*df, ax = None, cbar = False, normalize = None, folds = 10, shuffle = True, seed = 42, **kargs*)<a class='anchor' id='naivebayes'></a>\n",
    "#### A wrapper function that performs cross-validated evaluation of a Naive Bayes classifier on a given dataset\n",
    "\n",
    "The function has the following parameters:\n",
    "\n",
    "- **`df`**: Dataframe containing the data.\n",
    "- **`ax`**: *Axes object, default = None* \n",
    "     <br> Axes on which to draw the plot of the confusion matrix.\n",
    "- **`cbar`**: *bool, default = False*\n",
    "     <br> Whether or not to display the colorbar.\n",
    "- **`normalize`**: *None or {'true', 'pred', 'all'}*\n",
    "    <br> Normalization mode to apply to the confusion matrix.\n",
    "- **`folds`**: *int, default = 10*\n",
    "    <br> Number of folds to use in the k-fold cross-validation.\n",
    "- **`shuffle`**: *bool, default = True*\n",
    "    <br> Whether or not to shuffle the data before applying k-fold cross-validation.\n",
    "- **`seed`**: *int, default = 42*\n",
    "    <br> The random seed used to shuffle the data when 'shuffle' is set to True. If 'shuffle' is set to False, this parameter is ignored.\n",
    "- **`**kargs`**: \n",
    "    <br> Additional keyword arguments to pass to the GaussianNB() classifier.\n",
    "\n",
    "The function outputs a dictionary containing the following:\n",
    "\n",
    "- **`ConfusionMatrixDisplay`**: *ConfusionMatrixDisplay object*\n",
    "    <br> Object containing the confusion matrix, labels and the Confusion Matrix visualization.\n",
    "- **`confusionmatrix`**: *ndarray of shape (n_classes, n_classes)*\n",
    "  <br> Matrix whose *i*-th row and *j*-th column entry indicates the # of samples with true label being *i*-th class and predicted label being *j*-th class.\n",
    "- **`accuracy`**: *float*\n",
    "    <br>Mean cross-validated accuracy of the model.\n",
    "- **`recall`**: *float*\n",
    "    <br>Mean cross-validated percentage recall of the model.\n",
    "- **`model`**: *sklearn.pipeline.Pipeline object*\n",
    "    <br>Fitted pipeline object that includes the `GaussianNB` classifier and column transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "770b5f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbayes_cv(df, ax = None, cbar = False, normalize = None, folds = 10, shuffle = True, seed = 42, **kargs):\n",
    "    \n",
    "    # if no Axes object is provided, create a new one\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    # check if ax is an instance of the Axes class\n",
    "    elif not isinstance(ax, Axes):\n",
    "        raise TypeError(\"The 'ax' argument must be an Axes object\")\n",
    "    \n",
    "    # Separate the target variable from predictors\n",
    "    y = df['defects']\n",
    "    X = df.drop('defects', axis = 1)\n",
    "    predictors = X.columns.values.tolist()\n",
    "    \n",
    "    # Create StratifiedKFold object\n",
    "    # If shuffle is False, ignore any passed seed to avoid a ValueError \n",
    "    seed = seed if shuffle else None\n",
    "    kf = StratifiedKFold(n_splits = folds, shuffle = shuffle, random_state = seed)\n",
    "        \n",
    "    # create a Naive Bayes classifier with a Gaussian distribution assumption\n",
    "    nb = GaussianNB(**kargs)\n",
    "    \n",
    "    # Create column transformer to normalize predictors\n",
    "    z = make_column_transformer((RobustScaler(), predictors), remainder = \"passthrough\")\n",
    "    # Create pipeline\n",
    "    pipe = make_pipeline(z, nb)\n",
    "    \n",
    "    # calculate the mean cross-validated accuracy, percentage recall and AUC\n",
    "    scores = cross_validate(pipe, X, y, cv=kf, scoring = ('accuracy', 'recall', 'roc_auc'))\n",
    "    acc = np.mean(scores['test_accuracy'])\n",
    "    recall_perc = np.mean(scores['test_recall'])*100\n",
    "    auc = np.mean(scores['test_roc_auc'])\n",
    "    \n",
    "    # generate cross-validated predicted labels for the target class\n",
    "    y_pred = cross_val_predict(pipe, X, y, cv=kf)\n",
    "    \n",
    "    # create a ConfusionMatrixDisplay object using the confusion matrix and labels\n",
    "    lab = ['Clean','Buggy']\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(y_true=y, y_pred=y_pred, display_labels = lab, cmap='Blues',\n",
    "                                                   normalize = normalize, ax = ax, colorbar = cbar)\n",
    "    ax.set_title('GaussianNB', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Create output\n",
    "    out = {'ConfusionMatrixDisplay' : disp,\n",
    "           'confusionmatrix' : disp.confusion_matrix,\n",
    "           'accuracy' : round(acc, 3),\n",
    "           'recall' : round(recall_perc, 2),\n",
    "           'auc' : round(auc, 3),\n",
    "           'model' : pipe.fit(X, y)}\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f9117c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d554f",
   "metadata": {},
   "source": [
    "### knn_cv(*df, ax = None, cbar = False, normalize = None, folds = 10, shuffle = True, seed = 42, **kargs*)<a class='anchor' id='knn'></a>\n",
    "#### A wrapper function that performs cross-validated tuning and evaluation of a k-NN classifier on a given dataset\n",
    "\n",
    "The function has the following parameters:\n",
    "\n",
    "- **`df`**: Dataframe containing the data.\n",
    "- **`ax`**: *Axes object, default = None* \n",
    "     <br> Axes on which to draw the plot of the confusion matrix.\n",
    "- **`cbar`**: *bool, default = False*\n",
    "     <br> Whether or not to display the colorbar.\n",
    "- **`normalize`**: *None or {'true', 'pred', 'all'}*\n",
    "    <br> Normalization mode to apply to the confusion matrix.\n",
    "- **`folds`**: *int, default = 10*\n",
    "    <br> Number of folds to use in the k-fold cross-validation.\n",
    "- **`shuffle`**: *bool, default = True*\n",
    "    <br> Whether or not to shuffle the data before applying k-fold cross-validation.\n",
    "- **`seed`**: *int, default = 42*\n",
    "    <br> The random seed used to shuffle the data when `shuffle` is set to True. If `shuffle` is set to False, this parameter is ignored.\n",
    "- **`**kargs`**: \n",
    "    <br> Additional keyword arguments to pass to the `KNeighborsClassifier` classifier.\n",
    "\n",
    "The function outputs a dictionary containing the following:\n",
    "\n",
    "- **`ConfusionMatrixDisplay`**: *ConfusionMatrixDisplay object*\n",
    "    <br> Object containing the confusion matrix, labels and the Confusion Matrix visualization.\n",
    "- **`confusionmatrix`**: *ndarray of shape (n_classes, n_classes)*\n",
    "  <br> Matrix whose *i*-th row and *j*-th column entry indicates the # of samples with true label being *i*-th class and predicted label being *j*-th class.\n",
    "- **`results`**: *dict of ndarrays*\n",
    "    <br> a dictionary that summarizes the results of cross-validation for each hyperparameter combination tried during the grid search.\n",
    "- **`accuracy`**: *float*\n",
    "    <br>Mean cross-validated accuracy of the model.\n",
    "- **`recall`**: *float*\n",
    "    <br>Mean cross-validated percentage recall of the model.\n",
    "- **`model`**: *sklearn.pipeline.Pipeline object*\n",
    "    <br>The best estimator from `GridSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9182a6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_cv(df, ax = None, cbar = False, normalize = None, folds = 10, shuffle = True, seed = 42, **kargs):\n",
    "    \n",
    "    # If no Axes object is provided, create a new one\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    # Check if ax is an instance of the Axes class\n",
    "    elif not isinstance(ax, Axes):\n",
    "        raise TypeError(\"The 'ax' argument must be an Axes object\")\n",
    "        \n",
    "    # Separate the target variable from predictors\n",
    "    y = df['defects']\n",
    "    X = df.drop('defects', axis = 1)\n",
    "    predictors = X.columns.values.tolist()\n",
    "    \n",
    "    # Create StratifiedKFold object\n",
    "    # If shuffle is False, ignore any passed seed to avoid a ValueError\n",
    "    kf = StratifiedKFold(n_splits = folds, shuffle = shuffle, random_state = seed if shuffle else None)\n",
    "    \n",
    "    # Create k-NN model\n",
    "    knn = KNeighborsClassifier(n_jobs = -1, **kargs)\n",
    "    # Create column transformer to normalize predictors\n",
    "    z = make_column_transformer((RobustScaler(), predictors), remainder = \"passthrough\")\n",
    "    # Create pipeline\n",
    "    pipe = make_pipeline(z, knn)\n",
    "    \n",
    "    # Perform grid search to find the optimal number of neighbors\n",
    "    ks = {\"kneighborsclassifier__n_neighbors\": range(1,17,2)}\n",
    "    metrics = ('accuracy','balanced_accuracy','recall', 'roc_auc')\n",
    "    grid = GridSearchCV(pipe, ks, scoring = metrics, cv = kf, refit = 'balanced_accuracy', n_jobs = -1)\n",
    "    grid.fit(X, y)\n",
    "    k = grid.best_params_[\"kneighborsclassifier__n_neighbors\"]\n",
    "    \n",
    "    # Retrieve the mean cross-validated metrics\n",
    "    acc = grid.cv_results_['mean_test_accuracy'][grid.best_index_]\n",
    "    recall_perc = grid.cv_results_['mean_test_recall'][grid.best_index_]*100\n",
    "    auc = grid.cv_results_['mean_test_roc_auc'][grid.best_index_]\n",
    "    \n",
    "    # Generate cross-validated predicted labels for the target class\n",
    "    y_pred = cross_val_predict(grid.best_estimator_, X, y, cv = kf)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    lab = ['Clean','Buggy']\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(y_true = y, y_pred = y_pred,  cmap = 'Blues', display_labels = lab,\n",
    "                                                   normalize = normalize, ax = ax, colorbar = cbar)\n",
    "    ax.set_title(str(k) +'-NN', fontsize = 14, fontweight = 'bold')\n",
    "    \n",
    "    # Create output\n",
    "    out = {'ConfusionMatrixDisplay' : disp,\n",
    "           'confusionmatrix' : disp.confusion_matrix,\n",
    "           'results' : grid.cv_results_,\n",
    "           'accuracy' : round(acc, 3),\n",
    "           'recall' : round(recall_perc, 2),\n",
    "           'auc' : round(auc, 2),\n",
    "           'model' : grid.best_estimator_}\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63500d82",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9849de6",
   "metadata": {},
   "source": [
    "### tree_cv(*df, ax = None, cbar = False, normalize = None, folds = 10, shuffle = True, seed = 42, **kargs*) <a class='anchor' id='tree'></a>\n",
    "#### A wrapper function that performs cross-validated tuning and evaluation of a DecisionTreeClassifier on a given dataset\n",
    "\n",
    "The function has the following parameters:\n",
    "\n",
    "- **`df`**: Dataframe containing the data.\n",
    "- **`ax`**: *Axes object, default = None* \n",
    "     <br> Axes on which to draw the plot of the confusion matrix.\n",
    "- **`cbar`**: *bool, default = False*\n",
    "     <br> Whether or not to display the colorbar.\n",
    "- **`normalize`**: *None or {'true', 'pred', 'all'}*\n",
    "    <br> Normalization mode to apply to the confusion matrix.\n",
    "- **`folds`**: *int, default = 10*\n",
    "    <br> Number of folds to use in the k-fold cross-validation.\n",
    "- **`shuffle`**: *bool, default = True*\n",
    "    <br> Whether or not to shuffle the data before applying k-fold cross-validation.\n",
    "- **`seed`**: *int, default = 42*\n",
    "    <br> Integer used as a seed for the random number generator\n",
    "- **`**kargs`**: \n",
    "    <br> Additional keyword arguments to pass to the `DecisionTreeClassifier` classifier.\n",
    "\n",
    "The function outputs a dictionary containing the following:\n",
    "\n",
    "- **`ConfusionMatrixDisplay`**: *ConfusionMatrixDisplay object*\n",
    "    <br> Object containing the confusion matrix, labels and the Confusion Matrix visualization.\n",
    "- **`confusionmatrix`**: *ndarray of shape (n_classes, n_classes)*\n",
    "  <br> Matrix whose *i*-th row and *j*-th column entry indicates the # of samples with true label being *i*-th class and predicted label being *j*-th class.\n",
    "- **`results`**: *dict of ndarrays*\n",
    "    <br> a dictionary that summarizes the results of cross-validation for each hyperparameter combination tried during the grid search.\n",
    "- **`accuracy`**: *float*\n",
    "    <br>Mean cross-validated accuracy of the model.\n",
    "- **`recall`**: *float*\n",
    "    <br>Mean cross-validated percentage recall of the model.\n",
    "- **`model`**: *sklearn.pipeline.Pipeline object*\n",
    "    <br>The best estimator from `GridSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "558a257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_cv(df, ax = None, cbar = False, normalize = None, folds = 10, shuffle = True, seed = 42, **kargs):\n",
    "    \n",
    "    # If no Axes object is provided, create a new one\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    # Check if ax is an instance of the Axes class\n",
    "    elif not isinstance(ax, Axes):\n",
    "        raise TypeError(\"The 'ax' argument must be an Axes object\")\n",
    "        \n",
    "    # Separate the target variable from predictors\n",
    "    y = df['defects']\n",
    "    X = df.drop('defects', axis = 1)\n",
    "    predictors = X.columns.values.tolist()\n",
    "    \n",
    "    # Create StratifiedKFold object\n",
    "    # If shuffle is False, ignore any passed seed to avoid a ValueError\n",
    "    kf = StratifiedKFold(n_splits = folds, shuffle = shuffle, random_state = seed if shuffle else None)\n",
    "    \n",
    "    # Create a DecisionTreeClassifier model\n",
    "    tree_clf = DecisionTreeClassifier(random_state = seed, **kargs)\n",
    "    \n",
    "    # Create column transformer to normalize predictors\n",
    "    z = make_column_transformer((RobustScaler(), predictors), remainder = \"passthrough\")\n",
    "    # Create pipeline\n",
    "    pipe = make_pipeline(z, PCA(), tree_clf)\n",
    "    \n",
    "    # Get depth of the fully grown tree\n",
    "    pipe.fit(X, y)\n",
    "    full_tree = pipe.named_steps['decisiontreeclassifier']\n",
    "    full_depth = full_tree.get_depth()\n",
    "    \n",
    "    # Perform grid search to find the optimal depth\n",
    "    depths = {\"decisiontreeclassifier__max_depth\": range(1, full_depth, 2)}\n",
    "    metrics = ('accuracy','balanced_accuracy','recall', 'roc_auc')\n",
    "    grid = GridSearchCV(pipe, depths, scoring = metrics, cv = kf, refit = 'balanced_accuracy', n_jobs = -1)\n",
    "    grid.fit(X, y)\n",
    "    \n",
    "    # Retrieve the mean cross-validated metrics\n",
    "    acc = grid.cv_results_['mean_test_accuracy'][grid.best_index_]\n",
    "    recall_perc = grid.cv_results_['mean_test_recall'][grid.best_index_]*100\n",
    "    auc = grid.cv_results_['mean_test_roc_auc'][grid.best_index_]\n",
    "    \n",
    "    # Generate cross-validated predicted labels for the target class\n",
    "    y_pred = cross_val_predict(grid.best_estimator_, X, y, cv=kf)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    lab = ['Clean','Buggy']\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(y_true=y, y_pred=y_pred,  cmap='Blues', display_labels = lab, \n",
    "                                                   normalize = normalize, ax = ax, colorbar = cbar)\n",
    "    title = 'Balanced DecisionTree' if tree_clf.class_weight == 'balanced' else 'DecisionTree'\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Create output\n",
    "    out = {'ConfusionMatrixDisplay' : disp,\n",
    "           'confusionmatrix' : disp.confusion_matrix,\n",
    "           'results' : grid.cv_results_,\n",
    "           'accuracy' : round(acc, 3),\n",
    "           'recall' : round(recall_perc, 2),\n",
    "           'auc' : round(auc, 3),\n",
    "           'model' : grid.best_estimator_}\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138d809d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfff10b9",
   "metadata": {},
   "source": [
    "### plot_line(*axis, slope, intercept, **kargs*)<a class='anchor' id='plot_line'></a>\n",
    "#### Utility function for plotting a line on a given matplotlib axis object\n",
    "The plot_line function is a utility function for plotting a line on a given matplotlib axis object. The line is defined by its slope and intercept values, and it is plotted using the plt.plot function from the matplotlib library. The function takes at least three arguments:\n",
    "<br>\n",
    "- **`axis`**: a matplotlib axis object on which the line will be plotted.\n",
    "- **`slope`**: a float value that defines the slope of the line.\n",
    "- **`intercept`**: a float value that defines the y-intercept of the line.\n",
    "- **`kargs`**: a dictionary of optional keyword arguments that will be passed to the `plt.plot` function. This allows the caller to specify additional properties of the line such as its color, line style, and so on.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb15b4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line(axis, slope, intercept, **kargs):\n",
    "    xmin, xmax = axis.get_xlim()\n",
    "    plt.plot([xmin, xmax],\n",
    "             [xmin*slope+intercept, xmax*slope+intercept],\n",
    "             **kargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9a33f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30b3f86",
   "metadata": {},
   "source": [
    "### plot_performance(*data, metric_name, save=True, fig_num = None*) <a class='anchor' id='plotperf'></a>\n",
    "#### Creates and saves two plots (a boxplot and a barplot) of a given performance metric for classifiers on multiple datasets\n",
    "\n",
    "The `plot_classifier_performance` function takes in three inputs:\n",
    "\n",
    "- **`data`:** *pandas DataFrame*<br> contains the performance metric values for each classifier\n",
    "- **`metric_name`:** *str*<br> name of the performance metric being plotted\n",
    "- **`fig_num`:** *int, optional*<br> number to use as the figure number, default is None\n",
    "\n",
    "The function outputs two plots: a box plot and a bar plot, each showing the distribution of the performance metric for each classifier. The function saves the plots as image files if `fig_num` is not `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6398a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(data, metric_name, save=True, fig_num = None):\n",
    "    \n",
    "    # Check if input data is a pandas dataframe\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        raise TypeError(\"'data' argument must be a pandas DataFrame\")\n",
    "        \n",
    "    # Check if metric name is a string\n",
    "    if not isinstance(metric_name, str):\n",
    "        raise TypeError(\"'metric_name' argument must be a string\")\n",
    "    \n",
    "    sns.set(style=\"darkgrid\")\n",
    "    sns.set_palette('tab10')\n",
    "    \n",
    "    # Create box plot\n",
    "    box_a = sns.boxplot(data=data, linewidth=2.0, width=0.7)\n",
    "    \n",
    "    # Get the figure and set its size\n",
    "    fig = box_a.get_figure()\n",
    "    fig.set_size_inches(10,5)\n",
    "    \n",
    "    # Create the title for the plot\n",
    "    # If fig_num is provided, add \"Figure X. \" to the title, where X is the fig_num\n",
    "    head = 'Figure ' + str(fig_num) + \".  \" if fig_num is not None else ''\n",
    "    box_a.set_title(head + metric_name + \" Measure for Classifiers\", y=1.05, fontsize=15, fontweight='bold')\n",
    "    \n",
    "    # Set the font size for the x-axis tick labels\n",
    "    box_a.axes.tick_params(axis='x', labelsize=14)\n",
    "    plt.ylabel(metric_name, fontsize=14, labelpad=20, fontweight='bold')\n",
    "    \n",
    "    # Save the figure, if save is set to True\n",
    "    if save:\n",
    "        save_fig(metric_name + '_Boxplot')\n",
    "    \n",
    "    # Create bar plot\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    data.plot(kind='bar', stacked=False, ax=ax, width=0.7, edgecolor='darkslategray')\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), fontsize=12)\n",
    "    plt.xlabel('Datasets', fontsize=13, labelpad=10, fontweight='bold')\n",
    "    plt.ylabel(metric_name, fontsize=14, labelpad=20, fontweight='bold')\n",
    "    ax.tick_params(axis='x', labelrotation=0, labelsize=12)\n",
    "    \n",
    "    # Create the title for the plot\n",
    "    # If fig_num is provided, add \"Figure X. \" to the title, where X is the fig_num + 1\n",
    "    head = 'Figure ' + str(int(fig_num)+1) + \".  \" if fig_num is not None else ''\n",
    "    plt.title(head +  metric_name + \" Measure for Classifiers\", y=1.05, fontsize=15, fontweight='bold')\n",
    "    ax.grid(axis='x')\n",
    "    \n",
    "    # Save the figure, if save is set to True\n",
    "    if save:\n",
    "        save_fig(metric_name + '_Barplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493efeee",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2657bfc8",
   "metadata": {},
   "source": [
    "### roc_cv(*classifier, X, y, cv = None, ax = None, chance = True, **kargs*)<a class='anchor' id='roc_cv'></a>\n",
    "#### Calculates the cross-validated ROC curve and AUC for a given classifier \n",
    "It does this by splitting the data into a specified number of folds using a stratified k-fold cross-validation method (or any other iterable yielding train-test splits as arrays of indices) and then fitting the classifier on the training set and predicting probabilities on the test set. The false positive rate (FPR) and TPR for the test set are calculated using the `roc_curve` function from scikit-learn's `metrics` module, and the TPR is interpolated at the mean FPR points. The AUC for the test set is calculated using the roc_auc_score function.\n",
    "\n",
    "The mean TPR and standard deviation of TPR across all folds are calculated, as well as the mean AUC and standard deviation of AUC across all folds. \n",
    "The function takes the following arguments:\n",
    "- **`classifier`**: a binary classifier object that has a `fit` and `predict_proba` method.\n",
    "- **`df`**: a DataFrame of feature values in which the last column is assumed to be the target labels variable. \n",
    "- **`cv`**: an optional cross-validation generator or iterator. If not provided, a stratified k-fold cross-validation with 10 splits is used.\n",
    "- **`ax`**: an optional matplotlib `Axes` object to plot the ROC curve on. If not provided, a new Axes object is created.\n",
    "- **`chance`**: Boolean indicating whether to plot the chance level. Default is True.\n",
    "- **`**kargs`**: optional keyword arguments that are passed to the plot function when drawing the ROC curve.\n",
    "\n",
    "The function returns a dictionary containing the following keys:\n",
    "\n",
    "- **`AUC`**: the mean AUC value across all folds.\n",
    "- **`AUC_std`**: the standard deviation of the AUC values across all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f343e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_cv(classifier, df, cv = None, ax = None, chance = True , **kargs):\n",
    "    \n",
    "    # If no cross-validation object is provided, use stratified k-fold with 10 splits\n",
    "    if cv is None:\n",
    "        cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        \n",
    "    # If no Axes object is provided, create a new one\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    # Initialize lists to store true positive rates and AUC values\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    # Generate an array of 100 evenly spaced points between 0 and 1\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    # Set the target column to be the last column in the dataframe\n",
    "    target_col = df.columns[-1]\n",
    "    # Set the feature columns to be all the columns except the last one\n",
    "    feature_cols = df.columns[:-1]\n",
    "\n",
    "    # Iterate over the folds of the cross-validation\n",
    "    for fold, (train, test) in enumerate(cv.split(df, df[target_col])):\n",
    "        # Split the data into training and test sets\n",
    "        X_train, y_train = df[feature_cols].iloc[train], df[target_col].iloc[train]\n",
    "        X_test, y_test = df[feature_cols].iloc[test], df[target_col].iloc[test]\n",
    "        \n",
    "        # Fit the classifier on the training set\n",
    "        classifier.fit(X_train, y_train)\n",
    "        # Predict the probabilities of the positive class for the test set\n",
    "        y_pred = classifier.predict_proba(X_test)[:, 1]\n",
    "        # Calculate the false positive rate and true positive rate for the test set\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "        # Interpolate the true positive rate at the mean false positive rate points\n",
    "        interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "        # Set the first element of the interpolated true positive rate to 0\n",
    "        interp_tpr[0] = 0.0\n",
    "        # Add the interpolated true positive rate to the list\n",
    "        tprs.append(interp_tpr)\n",
    "        # Add the AUC for the test set to the list\n",
    "        aucs.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "    # Calculate the mean and standard deviation of the true positive rates across all folds\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "\n",
    "    # Calculate the mean and standard deviation of the AUC values across all folds\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "\n",
    "    # Stores the Classifier name in a variable\n",
    "    if isinstance(classifier, Pipeline):\n",
    "        classifier_name = classifier.steps[-1][1].__class__.__name__ \n",
    "    else:\n",
    "        classifier_name = classifier.__class__.__name__\n",
    "    \n",
    "    # If the chance flag is set to True, the function plots a chance level line (y=x, AUC=0.5)\n",
    "    if chance:\n",
    "        ax.plot([0, 1], [0, 1], \"k--\", label=\"Chance Level (AUC = 0.5)\", alpha=0.4)\n",
    "    \n",
    "    # Plot the ROC curve on the Axes object\n",
    "    ax.plot(\n",
    "        mean_fpr,\n",
    "        mean_tpr,\n",
    "        label=classifier_name + r\" (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "        lw=2,\n",
    "        alpha=0.8, \n",
    "        **kargs\n",
    "    )\n",
    "    ax.set_xlabel(\"False Positive Rate (FPR)\", labelpad=20)\n",
    "    ax.set_ylabel(\"True Positive Rate (TPR)\", labelpad=20)\n",
    "    \n",
    "    \n",
    "    # Return the Cv AUC, and standard deviation of the AUC\n",
    "    return {'AUC':mean_auc, 'AUC_std':std_auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fb1e99",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae48f296",
   "metadata": {},
   "source": [
    "### save_fig(*fig_id, tight_layout = True, fig_extension = \"png\", resolution = 300*)<a class='anchor' id='save_fig'></a>\n",
    "#### utility function for saving a matplotlib figure to a file\n",
    "\n",
    "The function takes four arguments:\n",
    "<br>\n",
    "- **`fig_id`**: a string that specifies the name of the file to which the figure will be saved.\n",
    "- **`tight_layout`**: a boolean value that determines whether or not the figure's layout should be tight (i.e., should take up as little space as possible). The default value is True.\n",
    "- **`fig_extension`**: a string that specifies the file extension of the figure file. The default value is \"png\".\n",
    "- **`resolution`**: an integer that specifies the resolution of the figure in dots per inch (dpi). The default value is 300.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "082db4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    #print(\"Saving figure\", fig_id)\n",
    "    \n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9391e08",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c686661",
   "metadata": {},
   "source": [
    "### scatter_custom(*x, y, df, ax, **kargs*) <a class='anchor' id='scatter_cust'></a>\n",
    "#### Custom function for generating a scatter plot from a pandas.DataFrame\n",
    "It takes three required arguments:\n",
    "- **`x and y`**: strings representing the names of the columns in df that you want to use for the x- and y-axes of the plot, respectively.\n",
    "- **`df`**: should be a pandas.DataFrame containing the data you want to plot.\n",
    "- **`ax`**: an *Axes* object, which is a part of a Figure in `matplotlib`. It represents a single subplot in a grid of plots. The ax argument allows the user to specify which Axes object the plot should be created on.\n",
    "\n",
    "The function also accepts additional keyword arguments, which are passed to the `matplotlib.pyplot.scatter` function when generating the plot. This allows you to customize the appearance of the plot, such as by setting the marker size or color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88055efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_custom(x, y, df, ax = None, **kargs):\n",
    "    \n",
    "    # if no Axes object is provided, create a new one\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    # check if ax is an instance of the Axes class\n",
    "    elif not isinstance(ax, Axes):\n",
    "        # if ax is not an Axes object, raise an error\n",
    "        raise TypeError(\"The 'ax' argument must be an Axes object\")\n",
    "    \n",
    "    # create sets of the provided x and y column names, and the column names in df\n",
    "    varnames_set = set((x,y))\n",
    "    columns_set = set(cm1_mc.columns)\n",
    "\n",
    "    # check if the x and y column names are a subset of the column names in df\n",
    "    if not varnames_set.issubset(columns_set):\n",
    "        # if x and y are not a subset of df.columns, raise an error\n",
    "        raise ValueError(f\"Provide existing columns names\")\n",
    "    \n",
    "    # Create two DataFrames for clean and buggy data\n",
    "    df_buggy = df.loc[df['defects'] == True]\n",
    "    df_clean = df.loc[df['defects'] == False]\n",
    "    df_grouped = [df_clean, df_buggy]\n",
    "    color = {True:'#8b0000',False:'#00008b'}\n",
    "    marker = {True:'s',False:'o'}\n",
    "    labels = ['Clean', 'Buggy']\n",
    "    \n",
    "    # create the plot\n",
    "    for i in range(2):\n",
    "        # scatter plot of the clean and buggy data\n",
    "        ax.scatter(x,y, alpha=0.7, marker = marker[i],\n",
    "                   edgecolors = color[i], color = 'white', data = df_grouped[i],\n",
    "                   label = labels[i], **kargs)\n",
    "        \n",
    "    # set the title, x-axis label, and y-axis label for the plot\n",
    "    # ax.set_title(), ax.set_xlabel(), and ax.set_ylabel()\n",
    "    ax.set_title((x + ' vs. ' + y), fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel(x, fontstyle='italic')\n",
    "    ax.set_ylabel(y, fontstyle='italic')\n",
    "    # show the legend for the plot\n",
    "    ax.legend()\n",
    "    # display the plot\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769573ac",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f3bed3",
   "metadata": {},
   "source": [
    "### svm_cv(*df, ax = None, cbar = False, normalize = None, folds = 10, shuffle = True, seed = 42, n_iter = 50, **kargs*) <a class='anchor' id='svm'></a>\n",
    "#### A wrapper function that performs cross-validated tuning and evaluation of a SVM with RBF kernel on a given dataset\n",
    "\n",
    "The function implements a randomized search over the parameter space using `RandomizedSearchCV`. In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the `n_iter` parameter. This way, increasing `n_iter` will always lead to a finer search.\n",
    "\n",
    "For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified. \n",
    "For continuous parameters, such as C in this case, it is important to specify a continuous distribution to take full advantage of the randomization. A continuous log-uniform random variable is available through `loguniform`. This is a continuous version of log-spaced parameters and is useful for searching penalty values as we often explore values at different orders of magnitude, at least as a first step. For example to specify C above, `loguniform(1, 100)` can be used instead of `[1, 10, 100]` \n",
    "\n",
    "The function has the following parameters:\n",
    "\n",
    "- **`df`**: Dataframe containing the data.\n",
    "- **`ax`**: *Axes object, default = None* \n",
    "     <br> Axes on which to draw the plot of the confusion matrix.\n",
    "- **`cbar`**: *bool, default = False*\n",
    "     <br> Whether or not to display the colorbar.\n",
    "- **`normalize`**: *None or {'true', 'pred', 'all'}*\n",
    "    <br> Normalization mode to apply to the confusion matrix.\n",
    "- **`folds`**: *int, default = 10*\n",
    "    <br> Number of folds to use in the k-fold cross-validation.\n",
    "- **`shuffle`**: *bool, default = True*\n",
    "    <br> Whether or not to shuffle the data before applying k-fold cross-validation.\n",
    "- **`seed`**: *int, default = 42*\n",
    "    <br> Integer used as a seed for the random number generator\n",
    "- **`n_iter`**: *int, default = 50*\n",
    "    <br> Number of parameter settings that are sampled. Tunes the trade-off runtime vs quality of the solution.\n",
    "- **`**kargs`**: \n",
    "    <br> Additional keyword arguments to pass to the `SVC` classifier.\n",
    "\n",
    "The function outputs a dictionary containing the following:\n",
    "\n",
    "- **`ConfusionMatrixDisplay`**: *ConfusionMatrixDisplay object*\n",
    "    <br> Object containing the confusion matrix, labels and the Confusion Matrix visualization.\n",
    "- **`confusionmatrix`**: *ndarray of shape (n_classes, n_classes)*\n",
    "  <br> Matrix whose *i*-th row and *j*-th column entry indicates the # of samples with true label being *i*-th class and predicted label being *j*-th class.\n",
    "- **`results`**: *dict of ndarrays*\n",
    "    <br> a dictionary that summarizes the results of cross-validation for each hyperparameter combination tried during the random search.\n",
    "- **`accuracy`**: *float*\n",
    "    <br>Mean cross-validated accuracy of the model.\n",
    "- **`recall`**: *float*\n",
    "    <br>Mean cross-validated percentage recall of the model.\n",
    "- **`model`**: *sklearn.pipeline.Pipeline object*\n",
    "    <br>The best estimator from `RandomizedSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26f7353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_cv(df, ax = None, cbar = False, normalize = None, folds = 10, shuffle = True, seed = 42, n_iter = 50, **kargs):\n",
    "    \n",
    "    # If no Axes object is provided, create a new one\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    # Check if ax is an instance of the Axes class\n",
    "    elif not isinstance(ax, Axes):\n",
    "        raise TypeError(\"The 'ax' argument must be an Axes object\")\n",
    "        \n",
    "    # Separate the target variable from predictors\n",
    "    y = df['defects']\n",
    "    X = df.drop('defects', axis = 1)\n",
    "    predictors = X.columns.values.tolist()\n",
    "    \n",
    "    # Create StratifiedKFold object\n",
    "    # If shuffle is False, ignore any passed seed to avoid a ValueError\n",
    "    kf = StratifiedKFold(n_splits = folds, shuffle = shuffle, random_state = seed if shuffle else None)\n",
    "\n",
    "    # Create a SupportVectorMachine model\n",
    "    svm = SVC(random_state = seed, cache_size = 1500, **kargs)\n",
    "    \n",
    "    # Create column transformer to normalize predictors\n",
    "    z = make_column_transformer((RobustScaler(), predictors), remainder = \"passthrough\")\n",
    "    # Create pipeline\n",
    "    pipe = make_pipeline(z, svm)\n",
    "    \n",
    "    # Perform Randomized Search to find the optimal parameters values\n",
    "    space = {'svc__C': loguniform(10e-3, 10e3),\n",
    "             'svc__gamma': loguniform(10e-3, 10e3)}\n",
    "    metrics = ('accuracy','balanced_accuracy','recall','roc_auc')\n",
    "    rnd_srch = RandomizedSearchCV(pipe, space, n_iter = n_iter, scoring = metrics, cv = kf, refit = 'balanced_accuracy',\n",
    "                                 n_jobs = -1, verbose = 0, random_state = seed)\n",
    "    rnd_srch.fit(X, y)\n",
    "    \n",
    "    # Retrieve the mean cross-validated metrics\n",
    "    acc = rnd_srch.cv_results_['mean_test_accuracy'][rnd_srch.best_index_]\n",
    "    recall_perc = rnd_srch.cv_results_['mean_test_recall'][rnd_srch.best_index_]*100\n",
    "    auc = rnd_srch.cv_results_['mean_test_roc_auc'][rnd_srch.best_index_]\n",
    "    \n",
    "    # Generate cross-validated predicted labels for the target class\n",
    "    y_pred = cross_val_predict(rnd_srch.best_estimator_, X, y, cv=kf)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    lab = ['Clean','Buggy']\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(y_true = y, y_pred = y_pred,  cmap = 'Blues', display_labels = lab, \n",
    "                                                   normalize = normalize, ax = ax, colorbar = cbar)\n",
    "    title = 'Balanced SVM' if svm.class_weight == 'balanced' else 'SVM'\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Create output\n",
    "    out = {'ConfusionMatrixDisplay' : disp,\n",
    "           'confusionmatrix' : disp.confusion_matrix,\n",
    "           'results' : rnd_srch.cv_results_,\n",
    "           'accuracy' : round(acc, 3),\n",
    "           'recall' : round(recall_perc, 2),\n",
    "           'auc' : round(auc, 3),\n",
    "           'model' : rnd_srch.best_estimator_}\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1193b022",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9bcfa3",
   "metadata": {},
   "source": [
    "### varselect_tocsv(*df, varnames, outname*) <a class='anchor' id='varselect_tocsv'></a>\n",
    "#### Selects features of interest and saves them in a CSV file\n",
    "\n",
    "This function is used to select a subset of columns (i.e., variables) from a Pandas dataframe and save the result as a CSV file. The function takes three arguments:\n",
    "\n",
    "- **`df`**: a Pandas dataframe that contains the data.\n",
    "- **`varnames`**: a list of strings, each of which is the name of a column in *df* that should be selected.\n",
    "- **`outname`**: a string that specifies the name to be used for the CSV file. This string should be a valid file name and will have *.csv* appended to it when the file is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b46647eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def varselect_tocsv(df, varnames, outname):\n",
    "    varnames_set = set(varnames)\n",
    "    columns_set = set(df.columns)\n",
    "    # Checks whether the list of variable names is a subset of the column names\n",
    "    if (isinstance(df, pd.core.frame.DataFrame) &\n",
    "        (varnames_set.issubset(columns_set)) &\n",
    "        (type(outname) == str)):\n",
    "            # Selects the specified columns from the dataframe\n",
    "            out_df = df[varnames]\n",
    "            # Saves the result to a CSV file using the specified name\n",
    "            # (any existing data is overwritten)\n",
    "            out_df.to_csv(f'{DATA_PATH}/{outname}.csv', mode=\"w\")\n",
    "            print('The variable selection was successfully saved in',f'{DATA_PATH}/{outname}.csv' )\n",
    "            return out_df\n",
    "    else:\n",
    "        # If any of the input arguments are invalid, the function prints an error message and returns None.\n",
    "        print('Provide proper arguments: a df, a list of feature names and an output name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3788a6b6",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db52fd6f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
