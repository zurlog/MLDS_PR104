{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa17919",
   "metadata": {},
   "source": [
    "# Utility Functions for PR104"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141fd883",
   "metadata": {},
   "source": [
    "1. [arff_tocsv()](#arff_tocsv)\n",
    "1. [data_barplot()](#barplot)\n",
    "1. [data_summary()](#data_summary)\n",
    "1. [naivebayes_cv()](#naivebayes)\n",
    "1. [knn_cv()](#knn)\n",
    "1. [plot_line()](#plot_line)\n",
    "1. [roc_cv()](#roc_cv)\n",
    "1. [save_fig()](#save_fig)\n",
    "1. [scatter_custom()](#scatter_cust)\n",
    "1. [varselect_tocsv()](#varselect_tocsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6fa7955",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a2e96b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b0b16f",
   "metadata": {},
   "source": [
    "### arff_tocsv(*name*) <a class='anchor' id='arff_tocsv'></a>\n",
    "#### Loads *.arff* data and save it in a CSV file\n",
    "This function is used to load data from an ARFF file and save it as a CSV file. The `with` statement is used to open the ARFF file, read its contents using the `loadarff` function, and automatically close the file when the with block ends. This ensures that the file is properly closed and avoids potential issues with file handles being left open.\n",
    "The function also checks if there is any missing value in the dataset and if there are missing values it impute the missing values with median values of the same column.\n",
    "It then prints a message indicating that the ARFF file was successfully loaded and saves the DataFrame as a CSV file using the `to_csv()` method. The CSV file is saved in the same directory as the ARFF file, using the same name as the ARFF file, overwriting any existing data. The function then returns the final dataframe.\n",
    "\n",
    "The function takes one argument:\n",
    "\n",
    "- **`name`**: a string that specifies the name of the ARFF file to be loaded. This string should be a valid file name and should have .arff appended to it when the file is read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "facdb089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.arff import loadarff \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def arff_tocsv(name):\n",
    "    # Constructs the full file path for the ARFF file\n",
    "    path = os.path.join(DATA_PATH, name + \".arff\")\n",
    "    \n",
    "    try:\n",
    "        # Reads the data from the file into a NumPy structured array\n",
    "        with open(path, \"r\") as arff_file:\n",
    "            raw_data = loadarff(arff_file)\n",
    "    except FileNotFoundError:\n",
    "        print(\"The specified ARFF file was not found.\")\n",
    "        return\n",
    "    \n",
    "    # Define the columns that should be of integer type\n",
    "    intdict = {'loc':'int','v(g)':'int','ev(g)':'int',\n",
    "               'iv(g)': 'int','n':'int','lOCode':'int',\n",
    "               'lOComment':'int','lOBlank':'int',\n",
    "               'lOCodeAndComment':'int','uniq_Op':'int',\n",
    "               'uniq_Opnd':'int','total_Op':'int',\n",
    "               'total_Opnd':'int','branchCount':'int',\n",
    "               'defects':'int'}\n",
    "    \n",
    "    # Array is then converted to a Pandas dataframe \n",
    "    df_data = pd.DataFrame(raw_data[0])\n",
    "    \n",
    "    # Check if there are any missing values in the dataframe\n",
    "    # 5 rows with missing values can be found in jm1.arff data source\n",
    "    if df_data.isnull().any().any():\n",
    "        # Impute missing values with median of the same column\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        df_data_num = df_data.select_dtypes(include=[np.number])\n",
    "        imputer.fit(df_data_num)\n",
    "        X = imputer.transform(df_data_num)\n",
    "        df_data = pd.DataFrame(X, columns=df_data.columns)\n",
    "    \n",
    "    # Convert the variables contained in the intdict dictionary to an integer dtype\n",
    "    df_data = df_data.astype(dtype=intdict)\n",
    "\n",
    "    print(name + \".arff\",\"successfully loaded\")\n",
    "    \n",
    "    # Saves the dataframe as a CSV file using the same name as the ARFF file\n",
    "    # (any existing data is overwritten)\n",
    "    df_data.to_csv(f'{DATA_PATH}/{name}.csv', mode=\"w\")\n",
    "    print(\"Saved in\", f'{DATA_PATH}/{name}.csv')\n",
    "    return df_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ed5366",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61951079",
   "metadata": {},
   "source": [
    "### data_barplot(*df, save = True, id='data_barplot', **kargs*) <a class='anchor' id='barplot'></a>\n",
    "#### Creates a grouped bar plot with the *Buggy* and *Clean* instances\n",
    "\n",
    "It plots the '*Buggy*' and '*Clean*' columns from `df`, with the '*Buggy*' bars shown in red and the '*Clean*' bars shown in blue. The x-axis of the plot is labeled with the index values from `df`, corresponding to the different datasets employed. The figure has a title, and is saved with the specified `id` before being shown. The `save_fig()` function is a separate function used to save the figure.\n",
    "The function takes four arguments:\n",
    "<br>\n",
    "- **`df`**: is the dataframe containing the data to be plotted\n",
    "- **`save`**: a boolean value indicating whether the plot should be saved (default is True)\n",
    "- **`id`**: a string value specifying the name of the file to save the plot as (default is 'data_barplot')\n",
    "- **`**kargs`**: any other keyword arguments to pass to the save_fig function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e49facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_barplot(df, save = True, id='data_barplot', **kargs):\n",
    "    \n",
    "    # Create x-axis values\n",
    "    num_rows = df.shape[0]\n",
    "    x = np.arange(num_rows)\n",
    "\n",
    "    y1 = df['Buggy']\n",
    "    y2 = df['Clean']\n",
    "    # Set width of bars in plot\n",
    "    width = 0.40\n",
    "\n",
    "    # Set tick positions on x-axis\n",
    "    ticks_pos = [r for r in range(num_rows)]\n",
    "\n",
    "    # Create figure and axis objects\n",
    "    figure, ax = plt.subplots(figsize=(7,4))\n",
    "\n",
    "    # Plot data in grouped bar format\n",
    "    ax.bar(x-0.2, y1, width, label='Buggy', color='#8b0000')\n",
    "    ax.bar(x+0.2, y2, width, label='Clean', color='#00008b')\n",
    "\n",
    "    # Set tick positions and labels on x-axis\n",
    "    ax.set_xticks(ticks_pos, df.index)\n",
    "\n",
    "    # Add legend to plot\n",
    "    ax.legend()\n",
    "\n",
    "    # Add title\n",
    "    figure.suptitle('Figure 2: Class Distribution among data - \"Defects\"',\n",
    "               fontsize=12, fontweight='bold')\n",
    "\n",
    "    # Save Figure\n",
    "    if save:\n",
    "        save_fig(id, **kargs)\n",
    "    \n",
    "    # Show plot\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed07b54",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370d3869",
   "metadata": {},
   "source": [
    "### data_summary(*filename, index, plot =* True)<a class='anchor' id='data_summary'></a>\n",
    "#### Reads the data from a specified file, creates a summary table and a grouped bar plot of the data.\n",
    "\n",
    "This function takes three arguments:\n",
    "<br>\n",
    "- **`filename`**: a string that specifies the name of the file containing the data. This function assumes that the file is a CSV file located in the `DATA_PATH` directory.\n",
    "- **`index`**: specifies the column to use as the index of the dataframe.\n",
    "- **`plot`**: a boolean that, if set to True, causes the function to also return a grouped bar plot of the data using the `data_barplot()` function. If plot is not provided, it defaults to True.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30d34846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_summary(filename, index, plot = True):\n",
    "    try:\n",
    "        # Check that the filename is a string\n",
    "        if not isinstance(filename, str):\n",
    "            raise ValueError('ERROR: The filename must be a string')\n",
    "\n",
    "        # Constructs the full file path for the CSV file\n",
    "        path = os.path.join(DATA_PATH, filename + \".csv\")\n",
    "\n",
    "        # Open the file with the given filename and read the data into a DataFrame\n",
    "        table1 = pd.read_csv(path)\n",
    "\n",
    "        # Check that the index is a proper column name\n",
    "        if index not in table1.columns:\n",
    "            raise ValueError('ERROR: The index must be an existing column')\n",
    "\n",
    "        # Set the 'Name' column as the index\n",
    "        table1.set_index(index, inplace=True)\n",
    "\n",
    "        # Add a new column called 'Clean' with the number of modules without bugs\n",
    "        table1.insert(2,'Clean',table1['Instances'] -  table1['Buggy'])\n",
    "\n",
    "        # Format the dataframe as an HTML table\n",
    "        html_table1 = table1.style.to_html(caption=\"Table 1: Summary of Datasets\")\n",
    "\n",
    "        # If the plot argument is True, create a grouped bar plot of the data\n",
    "        if plot:\n",
    "            return HTML(html_table1), data_barplot(table1)\n",
    "        else:\n",
    "            # Otherwise, only return the HTML table\n",
    "            return HTML(html_table1)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # If the file does not exist, print an error message\n",
    "        print('ERROR: The file does not exist')\n",
    "    except ValueError as error:\n",
    "        print(error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c31cb12",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1dd141",
   "metadata": {},
   "source": [
    "### naivebayes_cv(*df, ax = None, cbar = False, normalize = None, **kargs*)<a class='anchor' id='naivebayes'></a>\n",
    "#### A wrapper function that performs cross-validated evaluation of a Naive Bayes classifier on a given dataset\n",
    "\n",
    "The function has the following parameters:\n",
    "\n",
    "- **`df`**: A dataframe containing the data\n",
    "- **`ax`**: An axis object to plot the confusion matrix on (default is None)\n",
    "- **`cbar`**: A boolean value for displaying a colorbar (default is False)\n",
    "- **`normalize`**: A value for normalizing the confusion matrix (default is None)\n",
    "- **`**kargs`**: Additional keyword arguments passed to the `StratifiedKFold` function\n",
    "\n",
    "The function outputs a dictionary containing the following:\n",
    "\n",
    "- **`ax`**: The axis object on which the confusion matrix is plotted \n",
    "- **`confusionmatrix`**: The confusion matrix object\n",
    "- **`accuracy`**: The accuracy of the model\n",
    "- **`recall`**: The recall of the model\n",
    "- **`model`**: The Naive Bayes classifier with a Gaussian distribution assumption which is a `GaussianNB` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "770b5f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naivebayes_cv(df, ax=None, cbar = False, normalize = None, **kargs):\n",
    "    \n",
    "    # if no Axes object is provided, create a new one\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    # check if ax is an instance of the Axes class\n",
    "    elif not isinstance(ax, Axes):\n",
    "        raise TypeError(\"The 'ax' argument must be an Axes object\")\n",
    "    \n",
    "    # Separate the target variable from predictors\n",
    "    y = df['defects']\n",
    "    X = df.drop('defects', axis = 1)\n",
    "    predictors = X.columns.values.tolist()\n",
    "    \n",
    "    # Create StratifiedKFold object\n",
    "    kf = StratifiedKFold(**kargs)\n",
    "        \n",
    "    # create a GaussianNB object, which is a Naive Bayes classifier with a Gaussian distribution assumption\n",
    "    nb = GaussianNB()\n",
    "    \n",
    "    # Create column transformer to normalize predictors\n",
    "    z = make_column_transformer((RobustScaler(), predictors), remainder = \"passthrough\")\n",
    "    # Create pipeline\n",
    "    pipe = make_pipeline(z, nb)\n",
    "    \n",
    "    # calculate the mean cross-validated accuracy and percentage recall\n",
    "    acc = cross_val_score(nb, X, y, cv=kf, scoring='accuracy').mean()\n",
    "    recall_perc = cross_val_score(nb, X, y, cv=kf, scoring='recall').mean()*100\n",
    "    \n",
    "    # generate cross-validated predicted labels for the target class\n",
    "    y_pred = cross_val_predict(nb, X, y, cv=kf)\n",
    "    \n",
    "    # create a ConfusionMatrixDisplay object using the confusion matrix and labels\n",
    "    lab = ['Clean','Buggy']\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(y_true=y, y_pred=y_pred, display_labels = lab, cmap='Blues',\n",
    "                                                   normalize = normalize, ax = ax, colorbar = cbar)\n",
    "    ax.set_title('GaussianNB', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Create output\n",
    "    out = {'ax' : ax,\n",
    "          'confusionmatrix' : disp,\n",
    "          'accuracy' : acc,\n",
    "          'recall' : recall_perc,\n",
    "          'model' : pipe}\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f9117c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d554f",
   "metadata": {},
   "source": [
    "### knn_cv(*df, ax = None, cbar = False, normalize = None, **kargs*)<a class='anchor' id='knn'></a>\n",
    "#### A wrapper function that performs cross-validated evaluation of a k-NN classifier on a given dataset\n",
    "\n",
    "The function has the following parameters:\n",
    "\n",
    "- **`df`**: A dataframe containing the data\n",
    "- **`ax`**: An axis object to plot the confusion matrix on (default is None)\n",
    "- **`cbar`**: A boolean value for displaying a colorbar (default is False)\n",
    "- **`normalize`**: A value for normalizing the confusion matrix (default is None)\n",
    "- **`kargs`**: Additional keyword arguments passed to the StratifiedKFold function\n",
    "\n",
    "The function outputs a dictionary containing the following:\n",
    "\n",
    "- **`ax`**: The axis object on which the confusion matrix is plotted\n",
    "- **`confusionmatrix`**: The *ConfusionMatrixDisplay* object\n",
    "- **`accuracy`**: The accuracy of the model\n",
    "- **`recall`**: The recall of the model\n",
    "- **`model`**: The best estimator from the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9182a6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_cv(df, ax=None, cbar = False, normalize = None, **kargs):\n",
    "    \n",
    "    # If no Axes object is provided, create a new one\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    # Check if ax is an instance of the Axes class\n",
    "    elif not isinstance(ax, Axes):\n",
    "        raise TypeError(\"The 'ax' argument must be an Axes object\")\n",
    "        \n",
    "    # Separate the target variable from predictors\n",
    "    y = df['defects']\n",
    "    X = df.drop('defects', axis = 1)\n",
    "    predictors = X.columns.values.tolist()\n",
    "    \n",
    "    # Create StratifiedKFold object\n",
    "    kf = StratifiedKFold(**kargs)\n",
    "    \n",
    "    # Create k-NN model\n",
    "    knn = KNeighborsClassifier(weights='distance')\n",
    "    # Create column transformer to normalize predictors\n",
    "    z = make_column_transformer((RobustScaler(), predictors), remainder = \"passthrough\")\n",
    "    # Create pipeline\n",
    "    pipe = make_pipeline(z,knn)\n",
    "    \n",
    "    # Perform grid search to find the optimal number of neighbors\n",
    "    ks = {\"kneighborsclassifier__n_neighbors\": range(1,15)}\n",
    "    grid = GridSearchCV(pipe, ks, scoring = \"recall\", cv = kf, refit = True)\n",
    "    grid.fit(X, y)\n",
    "    k=grid.best_params_[\"kneighborsclassifier__n_neighbors\"]\n",
    "    \n",
    "    # Calculate the mean cross-validated accuracy and recall\n",
    "    acc = cross_val_score(grid, X, y, cv= kf, scoring='accuracy').mean()\n",
    "    recall_perc = cross_val_score(grid, X, y, cv=kf, scoring='recall').mean()*100\n",
    "    \n",
    "    # Generate cross-validated predicted labels for the target class\n",
    "    y_pred = cross_val_predict(grid, X, y, cv=kf)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    lab = ['Clean','Buggy']\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(y_true=y, y_pred=y_pred,  cmap='Blues', display_labels = lab,\n",
    "                                                   normalize = normalize, ax = ax, colorbar = cbar)\n",
    "    ax.set_title(str(k) +'-NN', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Create output\n",
    "    out = {'ax' : ax,\n",
    "          'confusionmatrix' : disp,\n",
    "          'accuracy' : acc,\n",
    "          'recall' : recall_perc,\n",
    "          'model' : grid.best_estimator_}\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63500d82",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfff10b9",
   "metadata": {},
   "source": [
    "### plot_line(*axis, slope, intercept, **kargs*)<a class='anchor' id='plot_line'></a>\n",
    "#### Utility function for plotting a line on a given matplotlib axis object\n",
    "The plot_line function is a utility function for plotting a line on a given matplotlib axis object. The line is defined by its slope and intercept values, and it is plotted using the plt.plot function from the matplotlib library. The function takes at least three arguments:\n",
    "<br>\n",
    "- **`axis`**: a matplotlib axis object on which the line will be plotted.\n",
    "- **`slope`**: a float value that defines the slope of the line.\n",
    "- **`intercept`**: a float value that defines the y-intercept of the line.\n",
    "- **`kargs`**: a dictionary of optional keyword arguments that will be passed to the `plt.plot` function. This allows the caller to specify additional properties of the line such as its color, line style, and so on.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb15b4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line(axis, slope, intercept, **kargs):\n",
    "    xmin, xmax = axis.get_xlim()\n",
    "    plt.plot([xmin, xmax],\n",
    "             [xmin*slope+intercept, xmax*slope+intercept],\n",
    "             **kargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9a33f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2657bfc8",
   "metadata": {},
   "source": [
    "### roc_cv(*classifier, X, y, cv = None, ax = None, **kargs*)<a class='anchor' id='roc_cv'></a>\n",
    "#### Calculates the cross-validated ROC curve and AUC for a given classifier \n",
    "It does this by splitting the data into a specified number of folds using a stratified k-fold cross-validation method (or any other iterable yielding train-test splits as arrays of indices) and then fitting the classifier on the training set and evaluating its performance on the test set in each fold.\n",
    "\n",
    "The function takes the following arguments:\n",
    "- **`classifier`**: a classifier object that has a fit and predict_proba method.\n",
    "- **`df`**: a DataFrame of feature values in which the last column is assumed to be the target labels variable \n",
    "- **`cv`**: an optional cross-validation generator or iterator. If not provided, a stratified k-fold cross-validation with 10 splits is used.\n",
    "- **`ax`**: an optional matplotlib Axes object to plot the ROC curve on. If not provided, a new Axes object is created.\n",
    "- **`**kargs`**: optional keyword arguments that are passed to the plot function when drawing the ROC curve.\n",
    "\n",
    "The function returns a list containing:\n",
    "\n",
    "- **`ax`**: the Axes object used to plot the ROC curve.\n",
    "- **`mean_auc`**: the mean AUC value across all folds.\n",
    "- **`std_auc`**: the standard deviation of the AUC values across all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f343e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_cv(classifier, df, cv = None, ax = None, **kargs):\n",
    "    \n",
    "    # If no cross-validation object is provided, use stratified k-fold with 10 splits\n",
    "    if cv is None:\n",
    "        cv = StratifiedKFold(n_splits=10)\n",
    "        \n",
    "    # If no Axes object is provided, create a new one\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    # Initialize lists to store true positive rates and AUC values\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    # Generate an array of 100 evenly spaced points between 0 and 1\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    # Set the target column to be the last column in the dataframe\n",
    "    target_col = df.columns[-1]\n",
    "    # Set the feature columns to be all the columns except the last one\n",
    "    feature_cols = df.columns[:-1]\n",
    "\n",
    "    # Iterate over the folds of the cross-validation\n",
    "    for fold, (train, test) in enumerate(cv.split(df, df[target_col])):\n",
    "        # Split the data into training and test sets\n",
    "        X_train, y_train = df[feature_cols].iloc[train], df[target_col].iloc[train]\n",
    "        X_test, y_test = df[feature_cols].iloc[test], df[target_col].iloc[test]\n",
    "        \n",
    "        # Fit the classifier on the training set\n",
    "        classifier.fit(X_train, y_train)\n",
    "        # Predict the probabilities for the test set\n",
    "        y_pred = classifier.predict_proba(X_test)[:, 1]\n",
    "        # Calculate the false positive rate and true positive rate for the test set\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "        # Interpolate the true positive rate at the mean false positive rate points\n",
    "        interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "        # Set the first element of the interpolated true positive rate to 0\n",
    "        interp_tpr[0] = 0.0\n",
    "        # Add the interpolated true positive rate to the list\n",
    "        tprs.append(interp_tpr)\n",
    "        # Add the AUC for the test set to the list\n",
    "        aucs.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "    # Calculate the mean and standard deviation of the true positive rates across all folds\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "\n",
    "    # Calculate the mean and standard deviation of the AUC values across all folds\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "\n",
    "    # Stores the Classifier name in a variable\n",
    "    if isinstance(classifier, Pipeline):\n",
    "        classifier_name = classifier.steps[-1][1].__class__.__name__ \n",
    "    else:\n",
    "        classifier_name = classifier.__class__.__name__\n",
    "    \n",
    "    # Plot the ROC curve on the Axes object\n",
    "    ax.plot(\n",
    "        mean_fpr,\n",
    "        mean_tpr,\n",
    "        label=classifier_name + r\" (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "        lw=2,\n",
    "        alpha=0.8, \n",
    "        **kargs\n",
    "    )\n",
    "    \n",
    "    # Return the Axes object, Cv AUC, and standard deviation of the AUC\n",
    "    return [ax, mean_auc, std_auc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fb1e99",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae48f296",
   "metadata": {},
   "source": [
    "### save_fig(*fig_id, tight_layout=True, fig_extension=\"png\", resolution=300*)<a class='anchor' id='save_fig'></a>\n",
    "#### utility function for saving a matplotlib figure to a file\n",
    "\n",
    "The function takes four arguments:\n",
    "<br>\n",
    "- **`fig_id`**: a string that specifies the name of the file to which the figure will be saved.\n",
    "- **`tight_layout`**: a boolean value that determines whether or not the figure's layout should be tight (i.e., should take up as little space as possible). The default value is True.\n",
    "- **`fig_extension`**: a string that specifies the file extension of the figure file. The default value is \"png\".\n",
    "- **`resolution`**: an integer that specifies the resolution of the figure in dots per inch (dpi). The default value is 300.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "082db4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    #print(\"Saving figure\", fig_id)\n",
    "    \n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9391e08",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c686661",
   "metadata": {},
   "source": [
    "### scatter_custom(*x, y, df, ax, **kargs*) <a class='anchor' id='scatter_cust'></a>\n",
    "#### Custom function for generating a scatter plot from a pandas.DataFrame\n",
    "It takes three required arguments:\n",
    "- **`x and y`**: strings representing the names of the columns in df that you want to use for the x- and y-axes of the plot, respectively.\n",
    "- **`df`**: should be a pandas.DataFrame containing the data you want to plot.\n",
    "- **`ax`**: an *Axes* object, which is a part of a Figure in `matplotlib`. It represents a single subplot in a grid of plots. The ax argument allows the user to specify which Axes object the plot should be created on.\n",
    "\n",
    "The function also accepts additional keyword arguments, which are passed to the `matplotlib.pyplot.scatter` function when generating the plot. This allows you to customize the appearance of the plot, such as by setting the marker size or color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88055efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_custom(x, y, df, ax = None, **kargs):\n",
    "    \n",
    "    # if no Axes object is provided, create a new one\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    # check if ax is an instance of the Axes class\n",
    "    if not isinstance(ax, Axes):\n",
    "        # if ax is not an Axes object, raise an error\n",
    "        raise TypeError(\"The 'ax' argument must be an Axes object\")\n",
    "    \n",
    "    # create sets of the provided x and y column names, and the column names in df\n",
    "    varnames_set = set((x,y))\n",
    "    columns_set = set(cm1_mc.columns)\n",
    "\n",
    "    # check if the x and y column names are a subset of the column names in df\n",
    "    if not varnames_set.issubset(columns_set):\n",
    "        # if x and y are not a subset of df.columns, raise an error\n",
    "        raise ValueError(f\"Provide existing columns names\")\n",
    "    \n",
    "    # Create two DataFrames for clean and buggy data\n",
    "    df_buggy = df.loc[df['defects'] == True]\n",
    "    df_clean = df.loc[df['defects'] == False]\n",
    "    df_grouped = [df_clean, df_buggy]\n",
    "    color = {True:'#8b0000',False:'#00008b'}\n",
    "    marker = {True:'s',False:'o'}\n",
    "    labels = ['Clean', 'Buggy']\n",
    "    \n",
    "    # create the plot\n",
    "    for i in range(2):\n",
    "        # scatter plot of the clean and buggy data\n",
    "        ax.scatter(x,y, alpha=0.7, marker = marker[i],\n",
    "                   edgecolors = color[i], color = 'white', data = df_grouped[i],\n",
    "                   label = labels[i], **kargs)\n",
    "        \n",
    "    # set the title, x-axis label, and y-axis label for the plot\n",
    "    # ax.set_title(), ax.set_xlabel(), and ax.set_ylabel()\n",
    "    ax.set_title((x + ' vs. ' + y), fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel(x, fontstyle='italic')\n",
    "    ax.set_ylabel(y, fontstyle='italic')\n",
    "    # show the legend for the plot\n",
    "    ax.legend()\n",
    "    # display the plot\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769573ac",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9bcfa3",
   "metadata": {},
   "source": [
    "### varselect_tocsv(*df, varnames, outname*) <a class='anchor' id='varselect_tocsv'></a>\n",
    "#### Selects features of interest and saves them in a CSV file\n",
    "\n",
    "This function is used to select a subset of columns (i.e., variables) from a Pandas dataframe and save the result as a CSV file. The function takes three arguments:\n",
    "\n",
    "- **`df`**: a Pandas dataframe that contains the data.\n",
    "- **`varnames`**: a list of strings, each of which is the name of a column in *df* that should be selected.\n",
    "- **`outname`**: a string that specifies the name to be used for the CSV file. This string should be a valid file name and will have *.csv* appended to it when the file is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b46647eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def varselect_tocsv(df, varnames, outname):\n",
    "    varnames_set = set(varnames)\n",
    "    columns_set = set(df.columns)\n",
    "    # Checks whether the list of variable names is a subset of the column names\n",
    "    if (isinstance(df, pd.core.frame.DataFrame) &\n",
    "        (varnames_set.issubset(columns_set)) &\n",
    "        (type(outname) == str)):\n",
    "            # Selects the specified columns from the dataframe\n",
    "            out_df = df[varnames]\n",
    "            # Saves the result to a CSV file using the specified name\n",
    "            # (any existing data is overwritten)\n",
    "            out_df.to_csv(f'{RESULTS_PATH}/{outname}.csv', mode=\"w\")\n",
    "            print('The variable selection was successfully saved in',f'{RESULTS_PATH}/{outname}.csv' )\n",
    "            return out_df\n",
    "    else:\n",
    "        # If any of the input arguments are invalid, the function prints an error message and returns None.\n",
    "        print('Provide proper arguments: a df, a list of feature names and an output name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3788a6b6",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db52fd6f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
